{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to do:\n",
    "* read documentation about what each parameter means in both models, also figure out which model\n",
    "* consider that many of these choices will likely be very different for large amount of data\n",
    "* figure out what convergence warning is\n",
    "* read slides and book and determine reasonable hyperparams to run tests\n",
    "* read piazza commentary\n",
    "* throw on AWS cloud\n",
    "* Swap to unigram and check for random state issue\n",
    "* use grid for parameter selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "1. Names of the two types of classifiers you opt to learn.\n",
    "* Neural Networks, Random Forest\n",
    "2. Proper citations for any external code you use. See https://integrity.mit.edu/handbook/writing-code for guidelines.\n",
    "* Cite Ski-kit documentation\n",
    "3. Description of your training methodology, with enough details so that another machine learning enthusiast can reproduce the your results.\n",
    "* Write about how you chose different hyperparameters, maybe make them adapt to the previous one??\n",
    "4. The final hyperparameter settings you use.\n",
    "* Neural Networks:\n",
    "    * Number of layers:\n",
    "    * Width of layer:\n",
    "    * Model:\n",
    "    * Other?\n",
    "* Random Forest:\n",
    "    * Number of trees:\n",
    "    * Other?\n",
    "5. Training error rates, hold-out or cross-validation error rates, and test error rates for your two final classifiers.\n",
    "* Neural Networks:\n",
    "    * Training error rates\n",
    "    * Hold out rates\n",
    "    * Test error rates\n",
    "* Random Forest:\n",
    "    * Training error rates\n",
    "    * Hold out rates\n",
    "    * Test error rates\n",
    "No need to submit any code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import scipy as sci\n",
    "from scipy import stats\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import vstack\n",
    "import time\n",
    "import pandas as pd\n",
    "import sklearn as sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "#read, split and select amount of data to work with\n",
    "samplerows = 50000 #100000 #1000000\n",
    "testrows =10000 #50000 #320122\n",
    "validations = 5\n",
    "data = pd.read_csv('reviews_tr.csv', nrows=samplerows, iterator=True)\n",
    "tdata = pd.read_csv('reviews_te.csv', nrows=testrows, iterator=True)\n",
    "\n",
    "#split data into text and labels\n",
    "dlabels = np.array(data['label'])\n",
    "dtext = data['text']\n",
    "tdlabels = np.array(tdata['label'])\n",
    "tdtext = tdata['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#using unigram representation\n",
    "#unigram represetation\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "uni_dtext = vectorizer.fit_transform(dtext)\n",
    "dictuni = vectorizer.get_feature_names()\n",
    "\n",
    "#unigram representation\n",
    "uni_dtext = vectorizer.fit_transform(dtext)\n",
    "dictuni = vectorizer.get_feature_names()\n",
    "vectorizerT = CountVectorizer(min_df=1, vocabulary=dictuni)\n",
    "uni_tdtext = vectorizerT.fit_transform(tdtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 550}\n",
      "(array([ 0.8240176 ,  0.8250175 ,  0.82691731]), array([ 0.8295,  0.83  ,  0.8284]), array([ 0.8329,  0.8325,  0.8321]), array([ 0.8299,  0.8326,  0.8318]), array([ 0.82888289,  0.83168317,  0.83048305]))\n",
      "('training:', 2e-05)\n",
      "('test:', 0.1635)\n",
      "--- 14890.3754358 seconds ---\n"
     ]
    }
   ],
   "source": [
    "datatotrain = uni_dtext\n",
    "labelstotrain = dlabels\n",
    "datatotest = uni_tdtext\n",
    "labelstotest = tdlabels\n",
    "\n",
    "#Random  forest\n",
    "clf = RandomForestClassifier( criterion='gini', max_depth=None, \\\n",
    "                                 min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \\\n",
    "                                 max_features='auto', max_leaf_nodes=None, min_impurity_split=1e-07, \\\n",
    "                                 bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, \\\n",
    "                                 warm_start=False, class_weight=None)\n",
    "\n",
    "param_grid = [\n",
    "  {'n_estimators': [500,550,600]}\n",
    "   ]\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=validations)\n",
    "\n",
    "grid_search.fit(datatotrain, labelstotrain)\n",
    "\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "#print(grid_search.cv_results_['split0_train_score'], grid_search.cv_results_['split1_train_score'], \\\n",
    "      #grid_search.cv_results_['split2_train_score'], grid_search.cv_results_['split3_train_score'], \\\n",
    "      #grid_search.cv_results_['split4_train_score'])\n",
    "print(grid_search.cv_results_['split0_test_score'], grid_search.cv_results_['split1_test_score'], \\\n",
    "      grid_search.cv_results_['split2_test_score'], grid_search.cv_results_['split3_test_score'], \\\n",
    "      grid_search.cv_results_['split4_test_score'])\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=grid_search.best_params_['n_estimators'], criterion='gini', max_depth=None, \\\n",
    "                                 min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \\\n",
    "                                 max_features='auto', max_leaf_nodes=None, min_impurity_split=1e-07, \\\n",
    "                                 bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, \\\n",
    "                                 warm_start=False, class_weight=None)\n",
    "clf.fit(datatotrain, labelstotrain)\n",
    "\n",
    "\n",
    "#check training error\n",
    "preds = clf.predict(datatotrain)\n",
    "error = (np.count_nonzero(np.abs(preds-labelstotrain)))/labelstotrain.shape[0]\n",
    "print(\"training:\", error)\n",
    "\n",
    "#run test error\n",
    "preds = clf.predict(datatotest)\n",
    "#check error \n",
    "error = (np.count_nonzero(np.abs(preds-labelstotest)))/labelstotest.shape[0]\n",
    "print(\"test:\", error)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rank_test_score': array([3, 2, 1], dtype=int32), 'split4_test_score': array([ 0.86168617,  0.87728773,  0.87388739]), 'std_test_score': array([ 0.01299976,  0.00538412,  0.00073667]), 'param_hidden_layer_sizes': masked_array(data = [(10, 100) (20, 100) (100, 100)],\n",
      "             mask = [False False False],\n",
      "       fill_value = ?)\n",
      ", 'split0_test_score': array([ 0.87381262,  0.86181382,  0.87491251]), 'mean_test_score': array([ 0.86372,  0.87178,  0.87388]), 'params': ({'solver': 'lbgfs', 'hidden_layer_sizes': (10, 100)}, {'solver': 'lbgfs', 'hidden_layer_sizes': (20, 100)}, {'solver': 'lbgfs', 'hidden_layer_sizes': (100, 100)}), 'split2_test_score': array([ 0.8751,  0.8741,  0.8736]), 'split3_test_score': array([ 0.8395,  0.8709,  0.8743]), 'param_solver': masked_array(data = ['lbgfs' 'lbgfs' 'lbgfs'],\n",
      "             mask = [False False False],\n",
      "       fill_value = ?)\n",
      ", 'split1_test_score': array([ 0.8685,  0.8748,  0.8727])}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'lbgfs', 'hidden_layer_sizes': (100, 100)}\n",
      "(array([ 0.87381262,  0.86181382,  0.87491251]), array([ 0.8685,  0.8748,  0.8727]), array([ 0.8751,  0.8741,  0.8736]), array([ 0.8395,  0.8709,  0.8743]), array([ 0.86168617,  0.87728773,  0.87388739]))\n",
      "('training:', 0.0796)\n",
      "('test:', 0.1226)\n",
      "--- 17674.348228 seconds ---\n"
     ]
    }
   ],
   "source": [
    "datatotrain = uni_dtext\n",
    "labelstotrain = dlabels\n",
    "datatotest = uni_tdtext\n",
    "labelstotest = tdlabels\n",
    "nfeatures = uni_dtext.shape[1]\n",
    "nclasses = 2\n",
    "av = 100 #int(np.round(nfeatures/nclasses, decimals=0))\n",
    "\n",
    "#NeuralNet\n",
    "clf = MLPClassifier(activation='relu', \\\n",
    "                        alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, \\\n",
    "                        power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, \\\n",
    "                        warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, \\\n",
    "                        validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "param_grid = [\n",
    "  {'hidden_layer_sizes': [(10,av),(20,av),(100,av)], 'solver': ['lbgfs']} \n",
    "    #,{'hidden_layer_sizes': [(2,50),(5,25),(10,12)], 'solver': ['adam']}\n",
    "   ]\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=validations)\n",
    "\n",
    "grid_search.fit(datatotrain, labelstotrain)\n",
    "\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "#print(grid_search.cv_results_['split0_train_score'], grid_search.cv_results_['split1_train_score'], \\\n",
    "      #grid_search.cv_results_['split2_train_score'], grid_search.cv_results_['split3_train_score'], \\\n",
    "      #grid_search.cv_results_['split4_train_score'])\n",
    "print(grid_search.cv_results_['split0_test_score'], grid_search.cv_results_['split1_test_score'], \\\n",
    "      grid_search.cv_results_['split2_test_score'], grid_search.cv_results_['split3_test_score'], \\\n",
    "      grid_search.cv_results_['split4_test_score'])\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=grid_search.best_params_['hidden_layer_sizes'], solver=grid_search.best_params_['solver'], \\\n",
    "                        activation='relu', \\\n",
    "                        alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, \\\n",
    "                        power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, \\\n",
    "                        warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, \\\n",
    "                        validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "clf.fit(datatotrain, labelstotrain)\n",
    "\n",
    "\n",
    "\n",
    "#check training error\n",
    "preds = clf.predict(datatotrain)\n",
    "error = (np.count_nonzero(np.abs(preds-labelstotrain)))/labelstotrain.shape[0]\n",
    "print(\"training:\", error)\n",
    "\n",
    "#run test error\n",
    "preds = clf.predict(datatotest)\n",
    "#check error \n",
    "error = (np.count_nonzero(np.abs(preds-labelstotest)))/labelstotest.shape[0]\n",
    "print(\"test:\", error)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
