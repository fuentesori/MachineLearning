{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import scipy as sci\n",
    "from scipy import stats\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import vstack\n",
    "import time\n",
    "import pandas as pd\n",
    "import sklearn as sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "#runing with 200,000 first rows of training data\n",
    "samplerows = 1000\n",
    "testrows = 1000\n",
    "validations =5\n",
    "data = pd.read_csv('reviews_tr.csv', nrows=samplerows, iterator=True)\n",
    "tdata = pd.read_csv('reviews_te.csv', nrows=testrows, iterator=True)\n",
    "\n",
    "#split data into text and labels\n",
    "dlabels = data['label']\n",
    "dtext = data['text']\n",
    "tdlabels = tdata['label']\n",
    "tdtext = tdata['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set up representations\n",
    "#unigram\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "#binary vectorizer for bayes\n",
    "vectorizerbinary = CountVectorizer(min_df=1, binary=True)\n",
    "#term frequency rep\n",
    "tfvect = TfidfVectorizer(smooth_idf=False)\n",
    "#bigram represetation\n",
    "vectorizer2 = CountVectorizer(min_df=1, ngram_range=(2,2))\n",
    "#drop common words\n",
    "vectorizer3 = CountVectorizer(min_df=1, stop_words=('the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'I', 'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create representations and dictionaries to fit test data\n",
    "#binary unigram representation for bayes (ie. 1s or 0s)\n",
    "unibi_dtext = vectorizerbinary.fit_transform(dtext)\n",
    "dictunibi = vectorizerbinary.get_feature_names()\n",
    "\n",
    "#unigram representation\n",
    "uni_dtext = vectorizer.fit_transform(dtext)\n",
    "dictuni = vectorizer.get_feature_names()\n",
    "\n",
    "#Term frequency-inverse document frequency tf-idf\n",
    "tf_dtext = tfvect.fit_transform(dtext)\n",
    "dicttf = tfvect.get_feature_names()\n",
    "\n",
    "#adjust to correct log\n",
    "tf_dtext = tf_dtext/(np.log(10))\n",
    "\n",
    "#bigram representation\n",
    "bi_dtext = vectorizer2.fit_transform(dtext)\n",
    "dictbi = vectorizer2.get_feature_names()\n",
    "\n",
    "#unigram removing 20 most common english words\n",
    "uni2_dtext = vectorizer3.fit_transform(dtext)\n",
    "dictuni2 = vectorizer3.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Perceptron (pdata, ptdata, plabels, ptlabels):\n",
    "#perceptron function\n",
    "    preds = []\n",
    "\n",
    "    w = np.zeros(pdata.shape[1])\n",
    "    u = np.zeros(pdata.shape[1])\n",
    "    b = 0\n",
    "    beta = 0\n",
    "    c = 0\n",
    "    \n",
    "    index = np.arange(0,(pdata.shape[0]))\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    for i in index:\n",
    "        if (plabels[i]==0):\n",
    "            y=-1\n",
    "        else:\n",
    "            y=1\n",
    "        x = (pdata[i]).toarray()\n",
    "        test = (y*(np.inner(w,x)+b))\n",
    "        if ((y*(np.inner(w,x)+b))< 0) or ((y*(np.inner(w,x)+b))== 0):\n",
    "            w = w + (y*x)\n",
    "            b = b +y\n",
    "            u = u + (y*c*x)\n",
    "            beta = beta + (y * c)\n",
    "        c = c +1\n",
    "\n",
    "    u = np.zeros(pdata.shape[1])\n",
    "    b = 0\n",
    "    beta = 0\n",
    "    c = 0        \n",
    "        \n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    for i in index:\n",
    "        if (plabels[i]==0):\n",
    "            y=-1\n",
    "        else:\n",
    "            y=1\n",
    "        x = (pdata[i]).toarray()\n",
    "        test = (y*(np.inner(w,x)+b))\n",
    "        if ((y*(np.inner(w,x)+b))< 0) or ((y*(np.inner(w,x)+b))== 0):\n",
    "            w = w + (y*x)\n",
    "            b = b +y\n",
    "            u = u + (y*c*x)\n",
    "            beta = beta + (y * c)\n",
    "        c = c +1\n",
    "\n",
    "    finalw = w - (1/c*u)\n",
    "    finalbeta = b - (1/c*beta)\n",
    "\n",
    "\n",
    "    #test perceptron\n",
    "    for i in range(ptdata.shape[0]):\n",
    "        x = (ptdata[i]).toarray()\n",
    "        newlabel = np.inner(finalw,x)\n",
    "        if newlabel <0 or newlabel == 0:\n",
    "            newlabel = 0\n",
    "        else:\n",
    "            newlabel = 1\n",
    "\n",
    "        preds.append(newlabel)\n",
    "\n",
    "    preds = np.array(preds)\n",
    "\n",
    "\n",
    "    error = check_error(preds,ptlabels)\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#subroutines for Naive Bayes\n",
    "def create_priors(priorslabels,classes):\n",
    "    indicesp = [a for a, x in enumerate(priorslabels) if x in [1]]\n",
    "    priors = [(len(priorslabels[indicesp]))/len(priorslabels)]\n",
    "    for i in range(0,classes-1):\n",
    "        indicesp = [a for a, x in enumerate(priorslabels) if x in [i]]\n",
    "        subprior = (len(priorslabels[indicesp]))/len(priorslabels)\n",
    "        priors.append(subprior)\n",
    "    priors= csr_matrix(np.log(np.array(priors)))\n",
    "    return priors\n",
    "\n",
    "def create_mus(usedata,uselabels,classes):\n",
    "    #create matrix of laplace smoothed mus - creates a 20 x 60k\n",
    "    indices = [a for a, x in enumerate(uselabels) if x in [0]]\n",
    "    collapse = usedata[indices].sum(axis=0)\n",
    "    collapse = csc_matrix((collapse +1)/(2+len(indices)))\n",
    "    for i in range(1,classes):\n",
    "        indices = [a for a, x in enumerate(uselabels) if x in [i]]\n",
    "        subdata1 = usedata[indices].sum(axis=0)\n",
    "        subdata1 = csc_matrix((subdata1 +1)/(2+len(indices)))\n",
    "        collapse = vstack([csc_matrix(collapse), subdata1],format=\"csc\")\n",
    "    return collapse\n",
    "\n",
    "def model(collapse,datamodel,priorsmodel,classes):\n",
    "    #create matrix of (log(1-mus)) - creates a 20 x 60k\n",
    "    minusmu = csc_matrix(np.log(1-(collapse.toarray())))\n",
    "    #create matrix of (logmu - log (1-mu)) - creates a 20 x 60k\n",
    "    minusmu2 = csc_matrix((np.log(collapse.toarray())) - minusmu)\n",
    "    #multiply data by minusmu2 and add to minusmu to obtain Prob(y=1)\n",
    "    firstprob = (datamodel.multiply(minusmu2[0])).sum(axis=1)\n",
    "    summedmu = (minusmu[0].sum(axis=1))+ priorsmodel[0,0]\n",
    "    allprobsY = csc_matrix(firstprob + summedmu)\n",
    "\n",
    "    for y in range(1,classes):\n",
    "        probY = (datamodel.multiply(minusmu2[y])).sum(axis=1)\n",
    "        summedmu = (minusmu[y].sum(axis=1)) + priorsmodel[0,y]\n",
    "        probY = csc_matrix(probY + summedmu)\n",
    "        allprobsY = hstack([allprobsY, probY],format=\"csc\") \n",
    "\n",
    "    preds = (np.argmax(allprobsY.toarray(),axis=1))\n",
    "    return preds\n",
    "\n",
    "def check_error(preds,labelscheck):\n",
    "    check = ((preds.astype(np.int8))) - (labelscheck.astype(np.int8))\n",
    "    error = (np.sum(check.astype(np.bool)))/len(preds)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#naive bayes classifier\n",
    "def NaiveBayes (datatrained, datatested, labelstrained, labelstested, classes):\n",
    "    priors = create_priors(labelstrained,classes)\n",
    "    collapse = create_mus(datatrained,labelstrained,classes)\n",
    "    preds = model(collapse,datatested,priors,classes)\n",
    "    error = check_error(preds,labelstested)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25   0.22   0.26   0.43   0.2  ]\n",
      " [ 0.29   0.18   0.255  0.31   0.21 ]\n",
      " [ 0.335  0.225  0.305  0.43   0.25 ]\n",
      " [ 0.31   0.225  0.245  0.285  0.23 ]\n",
      " [ 0.25   0.23   0.28   0.335  0.225]]\n",
      "[ 0.287  0.216  0.269  0.358  0.223]\n",
      "1\n",
      "--- 45.54667568206787 seconds ---\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(samplerows, n_folds=validations)\n",
    "errormatrix=[]\n",
    "for train_index, test_index in kf:\n",
    "    \n",
    "    crossdlabels = np.array(dlabels[train_index])\n",
    "    crosstdlabels = np.array(dlabels[test_index])\n",
    "    #run unigram that is binary on bayes\n",
    "    crossdtext = unibi_dtext[train_index]\n",
    "    crosstdtext = unibi_dtext[test_index]\n",
    "    \n",
    "    error = NaiveBayes(crossdtext, crosstdtext, crossdlabels, crosstdlabels,2)\n",
    "    errormatrix.append(error)\n",
    "    #run unigram on perceptron\n",
    "    crossdtext = uni_dtext[train_index]\n",
    "    crosstdtext = uni_dtext[test_index]\n",
    "     \n",
    "    error = Perceptron(crossdtext, crosstdtext, crossdlabels, crosstdlabels)\n",
    "    errormatrix.append(error)\n",
    "    #run bigram on perceprton\n",
    "    crossdtext = bi_dtext[train_index]\n",
    "    crosstdtext = bi_dtext[test_index]\n",
    "     \n",
    "    error = Perceptron(crossdtext, crosstdtext, crossdlabels, crosstdlabels)\n",
    "    errormatrix.append(error)\n",
    "    #run tf on perceptron\n",
    "    crossdtext = tf_dtext[train_index]\n",
    "    crosstdtext = tf_dtext[test_index]\n",
    "     \n",
    "    error = Perceptron(crossdtext, crosstdtext, crossdlabels, crosstdlabels)\n",
    "    errormatrix.append(error)\n",
    "    #run ex 20 common names on perceptron\n",
    "    crossdtext = uni2_dtext[train_index]\n",
    "    crosstdtext = uni2_dtext[test_index]\n",
    "     \n",
    "    error = Perceptron(crossdtext, crosstdtext, crossdlabels, crosstdlabels)\n",
    "    errormatrix.append(error)\n",
    "\n",
    "errormatrix = np.reshape(errormatrix,(validations,5))\n",
    "print(errormatrix)\n",
    "averror = np.mean(errormatrix,axis=0)\n",
    "print(averror)\n",
    "method = np.argmin(averror)\n",
    "print(method)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error: 0.101\n",
      "test error: 0.229\n",
      "Method: 1 test error: 0.229\n"
     ]
    }
   ],
   "source": [
    "#test on test data\n",
    "finaldlabels = np.array(dlabels)\n",
    "finaltdlabels = np.array(dlabels)\n",
    "\n",
    "if method==0:\n",
    "    #binary vectorizer for bayes\n",
    "    finaldtext = unibi_dtext    \n",
    "    finaltdtext = unibi_dtext\n",
    "    \n",
    "    #training\n",
    "    error = NaiveBayes(finaldtext, finaltdtext, finaldlabels, finaltdlabels,2)\n",
    "    print(\"training error:\",error)   \n",
    "    \n",
    "    #test\n",
    "    vectorizerbinaryT = CountVectorizer(min_df=1, binary=True, vocabulary=dictunibi)\n",
    "    unibi_tdtext = vectorizerbinaryT.fit_transform(tdtext)\n",
    "    \n",
    "    finaltdtext = unibi_tdtext\n",
    "    finaltdlabels = np.array(tdlabels)\n",
    "    \n",
    "    error = NaiveBayes(finaldtext, finaltdtext, finaldlabels, finaltdlabels,2)\n",
    "    print(\"test error:\",error)\n",
    "    \n",
    "elif method==1:\n",
    "    #unigram\n",
    "    finaldtext = uni_dtext\n",
    "    finaltdtext = uni_dtext\n",
    "    \n",
    "    #training\n",
    "    error = Perceptron(finaldtext, finaltdtext, finaldlabels, finaltdlabels)\n",
    "    print(\"training error:\",error)\n",
    "    \n",
    "    #test\n",
    "    vectorizerT = CountVectorizer(min_df=1, vocabulary=dictuni)\n",
    "    uni_tdtext = vectorizerT.fit_transform(tdtext)\n",
    "    finaltdtext = uni_tdtext\n",
    "    finaltdlabels = np.array(tdlabels)\n",
    "    \n",
    "    error = Perceptron(finaldtext, finaltdtext, finaldlabels, finaltdlabels)\n",
    "    print(\"test error:\",error)\n",
    "    \n",
    "elif method ==2:\n",
    "    #bigram\n",
    "    finaldtext = bi_dtext\n",
    "    finaltdtext = bi_dtext\n",
    "    \n",
    "    #training\n",
    "    error = Perceptron(finaldtext, finaltdtext, finaldlabels, finaltdlabels)\n",
    "    print(\"training error:\",error)\n",
    "    \n",
    "    #test\n",
    "    vectorizer2T = CountVectorizer(min_df=1, vocabulary=dictbi)\n",
    "    bi_tdtext = vectorizer2T.fit_transform(tdtext)\n",
    "    finaltdtext = bi_tdtext\n",
    "    finaltdlabels = np.array(tdlabels)\n",
    "    \n",
    "    error = Perceptron(finaldtext, finaltdtext, finaldlabels, finaltdlabels)\n",
    "    print(\"test error:\",error)\n",
    "    \n",
    "elif method ==3:\n",
    "    #tf method\n",
    "    finaldtext = tf_dtext\n",
    "    finaltdtext = tf_dtext\n",
    "    \n",
    "    #training\n",
    "    error = Perceptron(finaldtext, finaltdtext, finaldlabels, finaltdlabels)    \n",
    "    print(\"training error:\",error)\n",
    "    \n",
    "    #test\n",
    "    tfvectT = TfidfVectorizer(smooth_idf=False, vocabulary=dicttf)\n",
    "    tf_tdtext = tfvectT.fit_transform(tdtext)\n",
    "    finaltdtext = tf_tdtext\n",
    "    finaltdlabels = np.array(tdlabels)\n",
    "    \n",
    "    error = Perceptron(finaldtext, finaltdtext, finaldlabels, finaltdlabels)\n",
    "    print(\"test error:\",error)\n",
    "    \n",
    "elif method ==4:\n",
    "    #common\n",
    "    finaldtext = uni2_dtext\n",
    "    finaltdtext = uni2_dtext\n",
    "    \n",
    "    #training\n",
    "    error = Perceptron(finaldtext, finaltdtext, finaldlabels, finaltdlabels)\n",
    "    print(\"training error:\",error)\n",
    "    \n",
    "    #test\n",
    "    vectorizer3T = CountVectorizer(min_df=1, stop_words=('the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'I', 'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at'), vocabulary=dictuni2)\n",
    "    uni2_tdtext = vectorizer3T.fit_transform(tdtext)\n",
    "    finaltdtext = uni2_tdtext\n",
    "    finaltdlabels = np.array(tdlabels)\n",
    "    \n",
    "    error = Perceptron(finaldtext, finaltdtext, finaldlabels, finaltdlabels)\n",
    "    print(\"test error:\",error)\n",
    "else:\n",
    "    error =0\n",
    "\n",
    "print(\"Method:\",method,\"test error:\",error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
